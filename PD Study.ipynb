{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PD Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, average_precision_score, confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset.csv\", sep = \";\")\n",
    "df['has_paid'] = df['has_paid'].astype(int)\n",
    "\n",
    "df_nna = df[df['default'].notna()]  # observations for which we have default values (Training + Validation set)\n",
    "\n",
    "df_na = df[df['default'].isna()]    # observations for which we dont have default values (Unlabeled data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uuid', 'default', 'account_amount_added_12_24m',\n",
       "       'account_days_in_dc_12_24m', 'account_days_in_rem_12_24m',\n",
       "       'account_days_in_term_12_24m', 'account_incoming_debt_vs_paid_0_24m',\n",
       "       'account_status', 'account_worst_status_0_3m',\n",
       "       'account_worst_status_12_24m', 'account_worst_status_3_6m',\n",
       "       'account_worst_status_6_12m', 'age', 'avg_payment_span_0_12m',\n",
       "       'avg_payment_span_0_3m', 'merchant_category', 'merchant_group',\n",
       "       'has_paid', 'max_paid_inv_0_12m', 'max_paid_inv_0_24m', 'name_in_email',\n",
       "       'num_active_div_by_paid_inv_0_12m', 'num_active_inv',\n",
       "       'num_arch_dc_0_12m', 'num_arch_dc_12_24m', 'num_arch_ok_0_12m',\n",
       "       'num_arch_ok_12_24m', 'num_arch_rem_0_12m',\n",
       "       'num_arch_written_off_0_12m', 'num_arch_written_off_12_24m',\n",
       "       'num_unpaid_bills', 'status_last_archived_0_24m',\n",
       "       'status_2nd_last_archived_0_24m', 'status_3rd_last_archived_0_24m',\n",
       "       'status_max_archived_0_6_months', 'status_max_archived_0_12_months',\n",
       "       'status_max_archived_0_24_months', 'recovery_debt',\n",
       "       'sum_capital_paid_account_0_12m', 'sum_capital_paid_account_12_24m',\n",
       "       'sum_paid_inv_0_12m', 'time_hours', 'worst_status_active_inv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_na.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perform EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>A.</b> Checking for correlation between the variables. There seems to be no \"near-perfect\" correlation, but there is significant correlation between some pairs of variables. This could be an issue for a model where high multicolinearity might yield biased model parameters such as Logistic regression. However, I decided to use a non-parametric model - XGBoost and will therefore keep all variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the correlation matrix\n",
    "\n",
    "df.dtypes\n",
    "df['uuid']=df['uuid'].astype('category').cat.codes\n",
    "df['merchant_category']=df['merchant_category'].astype('category').cat.codes\n",
    "df['merchant_group']=df['merchant_group'].astype('category').cat.codes\n",
    "df['name_in_email']=df['name_in_email'].astype('category').cat.codes\n",
    "\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(df.corr(), fignum=f.number)\n",
    "plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> B. </b> For each of the 15 variables with NAs, there seems to be significant difference in the Default Rate between the observations with NAs compared to observations without NAs. It seems that observations with NAs hold important information. Therefore, I decided to apply Weight of Evidence encoding, which is useful because it encodes all values in the dataset as well as NA values. That way I will use all available information and will not have to discard any observations, nore would I have to approximate NAs with some other statistic (e.g. with medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing if the observations with NAs have different Default Rates than the observations without NAs.\n",
    "\n",
    "nna_perc = list(df_nna.apply(lambda x: np.mean(df_nna['default'][x.notna()])))\n",
    "nna_def = list(df_nna.apply(lambda x: int(np.sum(df_nna['default'][x.notna()]))))\n",
    "nna_all = list(df_nna.apply(lambda x: len(df_nna['default'][x.notna()])))\n",
    "\n",
    "na_perc  = list(df_nna.apply(lambda x: np.mean(df_nna['default'][x.isna()])))\n",
    "na_def  = list(df_nna.apply(lambda x: int(np.sum(df_nna['default'][x.isna()]))))\n",
    "na_all  = list(df_nna.apply(lambda x: len(df_nna['default'][x.isna()])))\n",
    "cols = list(df_nna.columns)\n",
    "\n",
    "na_nna_compare = pd.DataFrame({'VarName': cols,\n",
    "                               '% Default Not NAs': nna_perc,\n",
    "                               'Count Default Not NAs': nna_def,\n",
    "                               'Count All Not NAs': nna_all,\n",
    "                               '% Default NAs': na_perc,\n",
    "                               'Count Default NAs': na_def,\n",
    "                               'Count All NAs': na_all})\n",
    "\n",
    "na_nna_compare.dropna(inplace=True)\n",
    "\n",
    "\n",
    "def p_value(a,b,c,d):\n",
    "    count = [a, b]      \n",
    "    nobs = [c, d]          \n",
    "    stat, pval = proportions_ztest(count, nobs)\n",
    "    #pval=str(a/c) +''+str(b/d)\n",
    "    return pval\n",
    "\n",
    "na_nna_compare['proportions test_p value'] = na_nna_compare.apply(lambda x: p_value(x['Count Default Not NAs'], x['Count Default NAs'], x['Count All Not NAs'], x['Count All NAs'] ), axis=1)\n",
    "na_nna_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>C.</b> When comparing the distributions of predictor variables between observations which defaulted against the ones which did not default, we observe obvious differences between distributions </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the histograms for all variables. We can see differences in distribution between observations with and without NA values\n",
    "\n",
    "df_nna['has_paid'] = df_nna['has_paid'].astype(int)\n",
    "#plt.setp(axa.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "for i in df_nna.columns[2:]:\n",
    "    print(str(i))\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    df_nna.loc[(df_nna[\"default\"] ==0)][i].hist( bins=10, ax=axes[0])\n",
    "    df_nna.loc[(df_nna[\"default\"] ==1)][i].hist( bins=10, ax=axes[1])\n",
    "    \n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xticklabels(ax.get_xticks(), rotation=90)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>A.</b>  \n",
    "WOE encoding can be applied on either categorical or binned numerical variables. I have therefore identified all numerical variables in the dataset and have binned them into 20 bins each, before applying WOE encoding. Large number of bins used ensures that not much information had been lost in this transformation. Binning the numerical variables is also standard practice in credit risk  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idetifying numerical and categorical variables in the dataset\n",
    "\n",
    "def num_cat(df):\n",
    "    df_num_cat = pd.DataFrame(columns=['Columns', 'Unique Val','Unique Val Count'])\n",
    "    for col in df.columns:\n",
    "        row = pd.DataFrame({'Columns': [col], 'Unique Val': [df_nna[col].unique(),], 'Unique Val Count': [len(df_nna[col].unique())]})\n",
    "        df_num_cat = pd.concat([df_num_cat,row])\n",
    "        \n",
    "    df_num_cat['Variable Type'] = np.where((df_num_cat['Unique Val Count'] <15) , 'Categorical', 'Numerical')\n",
    "    df_num_cat['Variable Type'] = np.where(df_num_cat['Columns'].isin(['merchant_category','merchant_group', 'name_in_email']), 'Categorical', df_num_cat['Variable Type'])\n",
    "    df_num_cat['Has NA'] = np.where(df_num_cat['Columns'].isin(list(na_nna_compare['VarName'])), 'NA', 'OK')\n",
    "\n",
    "    df_num_cat['Variable Type'] = np.where(df_num_cat['Columns'].isin(['uuid','default']), '/', df_num_cat['Variable Type'])\n",
    "    return df_num_cat\n",
    "\n",
    "df_num_cat = num_cat(df_nna) \n",
    "df_num_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>B.</b> In this step, I applied the WOE encoding on the training + validation dataset. After that, I used this encoded dataset as a mapping table in order to encode the unlabeled dataset. This procedure is done for 2 reasons: 1- WOE Encoding needs a response variable, and unlabeled dataset does not have a response variable   2- Transforming the unlabeled dataset together with training data would lead to data leakage.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TRAINING SET\n",
    "#### bin all numerical and apply WOE\n",
    "\n",
    "## binning function\n",
    "def num_bin(df_nna):\n",
    "    df_nna_bin = df_nna.copy()\n",
    "    df_num_cat = num_cat(df_nna)\n",
    "    \n",
    "    num_vars = list(df_num_cat['Columns'].loc[df_num_cat['Variable Type']=='Numerical'])\n",
    "\n",
    "    for i in num_vars:   \n",
    "        df_nna_bin[i] = pd.cut(df_nna_bin[i], bins=20, precision = 0)\n",
    "\n",
    "\n",
    "    df_nna_bin[df_nna.columns[2:]] = df_nna_bin[df_nna.columns[2:]].astype(str)\n",
    "    \n",
    "    return df_nna_bin, num_vars\n",
    "\n",
    "## WOE encoding function for training set \n",
    "def WOE(df_nna):\n",
    "        \n",
    "    data_set, _ = num_bin(df_nna)\n",
    "    data_targets = data_set['default']\n",
    "    data_index = data_set[['uuid','default']]\n",
    "    data_features = data_set.drop(['default','uuid'], axis=1)\n",
    "\n",
    "    columns = [col for col in data_features.columns]\n",
    "    woe_encoder = ce.WOEEncoder(cols=columns)\n",
    "    woe_encoded_data = woe_encoder.fit_transform(data_features[columns], data_targets).add_suffix('_woe')\n",
    "    data_features = data_features.join(woe_encoded_data)\n",
    "    woe_data = data_set.join(woe_encoded_data)\n",
    "    return woe_data, list(woe_encoded_data.columns)\n",
    "\n",
    "\n",
    "\n",
    "############ UNLABELED SET\n",
    "#### bin all numerical variables and apply WOE using the mapping from the training set.\n",
    "\n",
    "\n",
    "#### function to make a lookup table from the training set \n",
    "def lookup(df_nna):\n",
    "    \n",
    "    df_nna_bin, num_vars = num_bin(df_nna)\n",
    "    \n",
    "    col_name=[]\n",
    "    lower_bound=[]\n",
    "    upper_bound=[]\n",
    "    orig_value=[]\n",
    "\n",
    "    for i in num_vars:\n",
    "        for j in df_nna_bin[i].unique():\n",
    "            if j!= 'nan':\n",
    "                col_name.append(i)\n",
    "                lower_bound.append(float(j.split(',')[0].replace('(','')))\n",
    "                upper_bound.append(float(j.split(',')[1].replace(']','')))\n",
    "                orig_value.append(j)\n",
    "\n",
    "    lookup_table = pd.DataFrame({'col_name': col_name,'lower_bound': lower_bound,'upper_bound': upper_bound, 'orig_value':orig_value})  \n",
    "    return lookup_table\n",
    "\n",
    "\n",
    "####### function that maps the numerical values of the unlabeled dataset into bins, using a lookup table based on training set\n",
    "def map_range(lookup_table, col_name, value):\n",
    "    #print(value,col_name, type(value))\n",
    "        \n",
    "    if np.isnan(value) == True:\n",
    "        fff='nan'\n",
    "        \n",
    "    elif (type(value) == float or type(value) == int):\n",
    "        sub_table = lookup_table.loc[lookup_table['col_name']==col_name]\n",
    "        try:          \n",
    "            fff = sub_table[\"orig_value\"][(sub_table[\"lower_bound\"] <= value) & (sub_table[\"upper_bound\"] > value)] \n",
    "            fff =str(fff.values[0]) \n",
    "        except:\n",
    "            fff='nan'               \n",
    "    else:\n",
    "        fff='nan'\n",
    "        \n",
    "    return fff\n",
    "\n",
    "################### function that transforms unlabeled dataset data into WOE values, based on the mapping from training set\n",
    "print('runs for ~5 mins')\n",
    "def na_bin_WOE(df_na, df_nna):\n",
    "\n",
    "    df_nna_bin, num_vars = num_bin(df_nna)\n",
    "    df_nna_bin_woe, woe_cols = WOE(df_nna)\n",
    "    lookup_table = lookup(df_nna)\n",
    "\n",
    "    df_na_bin = df_na.copy()\n",
    "    for i in num_vars:\n",
    "        df_na_bin[i] = df_na_bin[i].apply(lambda x: map_range(lookup_table,i,x) )\n",
    "    df_na_bin[df_na.columns[2:]] = df_na_bin[df_nna.columns[2:]].astype(str)\n",
    "\n",
    "    ####\n",
    "\n",
    "    df_nna_bin_cols= list(df_nna_bin.columns)[2:]\n",
    "    woe_mapping = pd.DataFrame({'Cols': df_nna_bin_cols,'Cols_WOE': woe_cols})\n",
    "\n",
    "    df_na_bin_woe = df_na_bin.copy()\n",
    "    for i in range(0,len(woe_mapping)):\n",
    "        df_map = df_nna_bin_woe[[str(woe_mapping.Cols[i]),str(woe_mapping.Cols_WOE[i])]].drop_duplicates()\n",
    "\n",
    "        df_na_bin_woe = pd.merge(df_na_bin_woe, df_map, how=\"left\", on=[str(woe_mapping.Cols[i])])\n",
    "    \n",
    "    return df_na_bin_woe , df_nna_bin_woe, woe_cols, lookup_table\n",
    "\n",
    "\n",
    "df_na_bin_woe, df_nna_bin_woe, woe_cols, lookup_table = na_bin_WOE(df_na, df_nna)  # \n",
    "df_nna_bin, num_vars = num_bin(df_nna)\n",
    "\n",
    "## df_nna_bin_woe  -  final data used for training and validating the model (Training and Validation)\n",
    "## df_na_bin_woe   -  final data used to provide predictions for which we dont have Default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>A.</b> I decided to use XGBoost model because it has a good track record for unbalanced classification problems, and is very flexible in the sense it is able to capture higly non-linear behavior. Another popular alternative in credit risk is Logistic regression which in its default mode only captures lineard decision boundaries for which I believe (but did not prove) would not be appropriate for this particular classification problem. The evaluation metric chosen in \"Area under the Precision-Recall curve\", since this is a higly unbalanced dataset  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'scale_pos_weight': 70, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##Split the data into Training and Validation(10k observations)\n",
    "\n",
    "index_list = list(df_nna_bin_woe.index)\n",
    "sampled_list = list(random.sample(index_list, 10000))\n",
    "df_validation = df_nna_bin_woe.iloc[sampled_list]\n",
    "X_val = df_validation[woe_cols].values\n",
    "y_val = df_validation[['default']].values\n",
    "df_train = df_nna_bin_woe.drop(sampled_list).reset_index(drop=True) # excluding the validation set from the training set\n",
    "\n",
    "## bootstraping to balance out the train dataset, but I gave up on this because XGBoost resolves this with \"scale_pos_weight\"\n",
    "\n",
    "#df_default = df_train[df_train['default']==1]\n",
    "#index_list = list(df_default.index)\n",
    "#sampled_list = list(np.random.choice(index_list, 30000))\n",
    "#df_bootstrap = df_train.iloc[sampled_list]\n",
    "#df_balanced = pd.concat([df_train,df_bootstrap])\n",
    "\n",
    "df_balanced = df_train.copy()\n",
    "X_train = df_balanced[woe_cols].values\n",
    "y_train = df_balanced[['default']].values\n",
    "\n",
    "# define the parameter grid to search over a number of values\n",
    "# I only specify a few parameters here becasue the procedure is very time cosuming \n",
    "# but I have experimented it for varios combinations of parameters and this one proivides best performance\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01],\n",
    "    'max_depth': [9,10],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.9],\n",
    "    'scale_pos_weight':[70]  # ~ ratio of of Count(Default=0) / count(Default=1) , due to our unbalanced data\n",
    "    #'reg_alpha': [0.1, 0.5],\n",
    "    #'reg_lambda': [0.1, 0.5],\n",
    "}\n",
    "\n",
    "# create an XGBoost classifier object\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='aucpr')\n",
    "\n",
    "# create a GridSearchCV object to search over the parameter grid with 5 folds\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5) #, scoring='roc_auc'\n",
    "\n",
    "# fit the grid search to the Training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>B.</b> Model performance on the validation set is moderate. I assumed it is more important to correctly classify the true Default=1 observations, than to avoid incorrectly classifing the true Default=0 observations. Therefore the Recall metric should be more important than Precision here. The confusion matrix tells us that out of 150 true Default=1 observations, 110 have been correctly classified, out of 1237 total Default=1 predictions that were made. In other words, Recall=0.73 and Precision =0.08  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.89216\n",
      "Recall: 0.736\n",
      "Precision: 0.07238394964594808\n",
      "AUC-PR: 0.18293312204144094\n",
      "[[8696 1179]\n",
      " [  33   92]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions on Validation data \n",
    "\n",
    "y_pred_prob = grid_search.best_estimator_.predict_proba(X_val)[:, 1]\n",
    "y_pred = grid_search.best_estimator_.predict(X_val)\n",
    "\n",
    "y_true = y_val.tolist()\n",
    "y_true = [int(item) for sublist in y_true for item in sublist]\n",
    "\n",
    "auc_roc = roc_auc_score(y_true, y_pred_prob)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "prc = precision_score(y_true, y_pred)\n",
    "auc_pr = average_precision_score(y_true, y_pred_prob,)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)\n",
    "print('Recall:',rec)\n",
    "print('Precision:',prc)\n",
    "print('AUC-PR:',auc_pr)       \n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>C.</b> Manual confirmation of the results above</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "1179\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "incorrect =0\n",
    "total =0\n",
    "for i, j in zip(list(y_pred_prob>0.5),list(y_true)):\n",
    "    \n",
    "    if int(i)>0 and int(i)==j:\n",
    "        correct=correct+1\n",
    "        \n",
    "    if int(i)>0 and int(i)!=j:\n",
    "        incorrect=incorrect+1\n",
    "        \n",
    "    if int(i)>0:\n",
    "        total=total+1\n",
    "    \n",
    "print(correct)\n",
    "print(incorrect)\n",
    "print(total)\n",
    "# https://datascience.stackexchange.com/questions/16232/in-xgboost-would-we-evaluate-results-with-a-precision-recall-curve-vs-roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>D.</b> Forecasting the Probability of Default for the unlabeled data using the trained model </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "      <th>PD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6f6e6c6a-2081-4e6b-8eb3-4fd89b54b2d7</td>\n",
       "      <td>0.21288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f6f6d9f3-ef2b-4329-a388-c6a687f27e70</td>\n",
       "      <td>0.25898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e9c39869-1bc5-4375-b627-a2df70b445ea</td>\n",
       "      <td>0.21153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6beb88a3-9641-4381-beb6-c9a208664dd0</td>\n",
       "      <td>0.36383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bb89b735-72fe-42a4-ba06-d63be0f4ca36</td>\n",
       "      <td>0.77086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5c03bc63-ea65-4ffd-aa7b-95ea9a46db34</td>\n",
       "      <td>0.24040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>f8db22f4-9819-420c-abbc-9ddf1843176e</td>\n",
       "      <td>0.18772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>b22e21ea-b1b2-4df3-b236-0ff6d5fdc0d8</td>\n",
       "      <td>0.69723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>bafcab15-9898-479c-b729-c9dda7edb78f</td>\n",
       "      <td>0.24195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>ac88f18c-96a6-49bc-9e9d-a780225914af</td>\n",
       "      <td>0.26798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      UUID      PD\n",
       "0     6f6e6c6a-2081-4e6b-8eb3-4fd89b54b2d7 0.21288\n",
       "1     f6f6d9f3-ef2b-4329-a388-c6a687f27e70 0.25898\n",
       "2     e9c39869-1bc5-4375-b627-a2df70b445ea 0.21153\n",
       "3     6beb88a3-9641-4381-beb6-c9a208664dd0 0.36383\n",
       "4     bb89b735-72fe-42a4-ba06-d63be0f4ca36 0.77086\n",
       "...                                    ...     ...\n",
       "9995  5c03bc63-ea65-4ffd-aa7b-95ea9a46db34 0.24040\n",
       "9996  f8db22f4-9819-420c-abbc-9ddf1843176e 0.18772\n",
       "9997  b22e21ea-b1b2-4df3-b236-0ff6d5fdc0d8 0.69723\n",
       "9998  bafcab15-9898-479c-b729-c9dda7edb78f 0.24195\n",
       "9999  ac88f18c-96a6-49bc-9e9d-a780225914af 0.26798\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = df_na_bin_woe[woe_cols].values\n",
    "uuid = df_na_bin_woe.uuid.values.tolist()\n",
    "y_pred_prob = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "test_set_result = pd.DataFrame({'UUID': uuid,'PD': y_pred_prob})\n",
    "test_set_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_result.to_csv(\"./test_set_result.csv\" , index=False, sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SERIALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_na' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mid\u001b[39m(\u001b[43mdf_na\u001b[49m)) \u001b[38;5;66;03m#  NOT NEEDED FURTHER\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mid\u001b[39m(df_nna))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mid\u001b[39m(df_nna_bin))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_na' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "print(id(df_na)) #  NOT NEEDED FURTHER\n",
    "print(id(df_nna))\n",
    "print(id(df_nna_bin))\n",
    "print(id(df_na_bin_woe))  # NOT NEEDED FURTHER\n",
    "print(id(df_nna_bin_woe))\n",
    "print(id(lookup_table))\n",
    "print(id(num_vars))\n",
    "print(id(woe_cols))\n",
    "print(id(grid_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "\n",
    "with open('/home/aleksa/Documents/PD_prediction/df_nna.pkl', 'wb') as file:\n",
    "    pickle.dump(df_nna, file)\n",
    "    \n",
    "with open('/home/aleksa/Documents/PD_prediction/df_nna_bin.pkl', 'wb') as file:\n",
    "    pickle.dump(df_nna_bin, file)\n",
    "\n",
    "with open('/home/aleksa/Documents/PD_prediction/df_na_bin_woe.pkl', 'wb') as file:\n",
    "    pickle.dump(df_na_bin_woe, file)  \n",
    "    \n",
    "with open('/home/aleksa/Documents/PD_prediction/df_nna_bin_woe.pkl', 'wb') as file:\n",
    "    pickle.dump(df_nna_bin_woe, file)\n",
    "    \n",
    "with open('/home/aleksa/Documents/PD_prediction/lookup_table.pkl', 'wb') as file:\n",
    "    pickle.dump(lookup_table, file)\n",
    "    \n",
    "with open('/home/aleksa/Documents/PD_prediction/num_vars.pkl', 'wb') as file:\n",
    "    pickle.dump(num_vars, file)\n",
    "    \n",
    "with open('/home/aleksa/Documents/PD_prediction/woe_cols.pkl', 'wb') as file:\n",
    "    pickle.dump(woe_cols, file)\n",
    "    \n",
    "joblib.dump(grid_search, '/home/aleksa/Documents/PD_prediction/grid_search.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break\n",
    "\n",
    "with open(\"/home/aleksa/Documents/PD_prediction/df_nna.pkl\", \"rb\") as infile:\n",
    "    df_nna = pd.read_pickle(infile) \n",
    "    \n",
    "with open(\"/home/aleksa/Documents/PD_prediction/df_nna_bin.pkl\", \"rb\") as infile:\n",
    "    df_nna_bin = pd.read_pickle(infile) \n",
    "          \n",
    "with open(\"/home/aleksa/Documents/PD_prediction/df_na_bin_woe.pkl\", \"rb\") as infile:\n",
    "    df_na_bin_woe = pd.read_pickle(infile) \n",
    "          \n",
    "with open(\"/home/aleksa/Documents/PD_prediction/df_nna_bin_woe.pkl\", \"rb\") as infile:\n",
    "    df_nna_bin_woe = pd.read_pickle(infile) \n",
    "          \n",
    "with open(\"/home/aleksa/Documents/PD_prediction/lookup_table.pkl\", \"rb\") as infile:\n",
    "    lookup_table = pd.read_pickle(infile) \n",
    "          \n",
    "with open(\"/home/aleksa/Documents/PD_prediction/num_vars.pkl\", \"rb\") as infile:\n",
    "    num_vars = pd.read_pickle(infile) \n",
    "          \n",
    "with open(\"/home/aleksa/Documents/PD_prediction/woe_cols.pkl\", \"rb\") as infile:\n",
    "    woe_cols = pd.read_pickle(infile) \n",
    "          \n",
    "grid_search = joblib.load('/home/aleksa/Documents/PD_prediction/grid_search.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:klarna_env]",
   "language": "python",
   "name": "conda-env-klarna_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
